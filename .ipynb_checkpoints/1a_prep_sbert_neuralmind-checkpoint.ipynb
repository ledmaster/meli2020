{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import joblib as jb\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "import json\n",
    "import dask\n",
    "\n",
    "import itertools\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"./data/train.parquet\")\n",
    "test = pd.read_parquet(\"./data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_parquet(\"./data/item_data.parquet\")\n",
    "\n",
    "item_title_map = item_data[['title', 'item_id']].set_index(\"item_id\").squeeze().to_dict()\n",
    "item_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item_data):\n",
    "        self.item_data = item_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.item_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = self.item_data.iloc[index]['title']\n",
    "\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pretrained = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "model = SentenceTransformer(pretrained,  device='cuda')\n",
    "train_data = Dataset(item_data)\n",
    "train_loader = DataLoader(train_data, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embs_list = list()\n",
    "for data in tqdm.tqdm(train_loader):\n",
    "    embs = model.encode(data)\n",
    "    embs_list.append(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_np = np.vstack(embs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(embs_np, \"22a_embs_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(data=embs_np, ids=item_data['item_id'].values)\n",
    "index.createIndex(print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb_map = {t: embs_np[i] for i, t in enumerate(item_data['item_id'].values)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = 0\n",
    "hs = list()\n",
    "for elist, t in tqdm.tqdm(train[['user_history', 'item_bought']].values):\n",
    "    elist = json.loads(elist)\n",
    "    rep = list()\n",
    "    for e in elist:\n",
    "        if isinstance(e['event_info'], int):\n",
    "            rep.append(item_emb_map[e['event_info']])\n",
    "            #print(item_title_map[e['event_info']])\n",
    "    h = np.mean(rep, axis=0)\n",
    "    #hs.append(h)\n",
    "    #h = rep[0]\n",
    "    #t = item_emb_map[t]\n",
    "    \n",
    "    #print()\n",
    "    try:\n",
    "        k = index.knnQuery(h, k=50)\n",
    "\n",
    "        recall += int(t in set(k[0]))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    #for i,d in zip(k[0], k[1]):\n",
    "    #    print(d, item_title_map[i])\n",
    "    #print(recall)    \n",
    "    #print(int(t in k[0]))\n",
    "    #print()\n",
    "    \n",
    "    \n",
    "    #print(item_title_map[t])\n",
    "    #print(\"-\"*10+\"\\n\"*5)\n",
    "print(recall/train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall@10 - 0.13778097264275843\n",
    "recall@20 - 0.15457821731374785\n",
    "recall@100 - 0.18157240604797623\n",
    "recall@1000 - 0.18950632074992194\n",
    "recall cs = viewed - 0.29388401187908886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.saveIndex(\"22a_sbert_neuralmind.nms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, search_data):\n",
    "        self.search_data = search_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.search_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_index = self.search_data[index][0]\n",
    "        search = self.search_data[index][1]\n",
    "        #print(search)\n",
    "        return seq_index, search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "search_data = set()\n",
    "seq_index = 0\n",
    "for hist, bought in tqdm.tqdm(train[['user_history', 'item_bought']].values):\n",
    "    \n",
    "    for item in json.loads(hist):\n",
    "        i = item['event_info']\n",
    "        if item['event_type'] == 'search':\n",
    "            search_data.add((seq_index, i.lower()))   \n",
    "    seq_index += 1\n",
    "search_data = list(search_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pretrained = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "model = SentenceTransformer(pretrained,  device='cuda')\n",
    "train_data = Dataset(search_data)\n",
    "train_loader = DataLoader(train_data, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_index_embs_map = np.zeros((train.shape[0], 1024))\n",
    "res = list()\n",
    "\n",
    "for seq_ix, search in tqdm.tqdm(train_loader):\n",
    "    #print(seq_i\n",
    "    #print(search_list)\n",
    "    emb = model.encode(search)\n",
    "    seq_ix = seq_ix.numpy()\n",
    "    for i in range(emb.shape[0]):\n",
    "        res.append((seq_ix[i], emb[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter([e[0] for e in res])\n",
    "\n",
    "seq_index_embs_map = np.zeros((train.shape[0], 1024))\n",
    "for seqix, emb in tqdm.tqdm(res):\n",
    "    seq_index_embs_map[seqix, :] += emb\n",
    "\n",
    "for i in tqdm.tqdm(range(train.shape[0])):\n",
    "    seq_index_embs_map[i, :] /= ctr.get(i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(seq_index_embs_map, \"22a_embs_search_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "search_data = set()\n",
    "seq_index = 0\n",
    "for hist in tqdm.tqdm(test['user_history'].values):\n",
    "    \n",
    "    for item in json.loads(hist):\n",
    "        i = item['event_info']\n",
    "        if item['event_type'] == 'search':\n",
    "            search_data.add((seq_index, i.lower()))   \n",
    "    seq_index += 1\n",
    "search_data = list(search_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pretrained = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "model = SentenceTransformer(pretrained,  device='cuda')\n",
    "test_data = Dataset(search_data)\n",
    "test_loader = DataLoader(test_data, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_index_embs_map = np.zeros((train.shape[0], 1024))\n",
    "res = list()\n",
    "\n",
    "for seq_ix, search in tqdm.tqdm(test_loader):\n",
    "    #print(seq_i\n",
    "    #print(search_list)\n",
    "    emb = model.encode(search)\n",
    "    seq_ix = seq_ix.numpy()\n",
    "    for i in range(emb.shape[0]):\n",
    "        res.append((seq_ix[i], emb[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter([e[0] for e in res])\n",
    "\n",
    "seq_index_embs_map = np.zeros((test.shape[0], 1024))\n",
    "for seqix, emb in tqdm.tqdm(res):\n",
    "    seq_index_embs_map[seqix, :] += emb\n",
    "\n",
    "for i in tqdm.tqdm(range(test.shape[0])):\n",
    "    seq_index_embs_map[i, :] /= ctr.get(i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(seq_index_embs_map, \"22a_embs_search_test_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
